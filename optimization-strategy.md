# Video-Service: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è

## –ß–∞—Å—Ç—å 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

#### üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ #1: CUDA –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
- **–í–ª–∏—è–Ω–∏–µ**: 82% –≤—Ä–µ–º–µ–Ω–∏ (311s –∏–∑ 385s) –Ω–∞ STT
- **–ü—Ä–∏—á–∏–Ω–∞**: `‚ö†Ô∏è CUDA requested but not available, falling back to CPU`
- **–í–∏–Ω–æ–≤–Ω–∏–∫**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch –∏–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç CUDA –≤–µ—Ä—Å–∏–π
- **–†–µ—à–µ–Ω–∏–µ**: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π CUDA –≤–µ—Ä—Å–∏–µ–π

#### üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ #2: –£–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω scores (0.53-0.57)
- **–í–ª–∏—è–Ω–∏–µ**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–ª–∏—á–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –∫–ª–∏–ø—ã –æ—Ç –ø–ª–æ—Ö–∏—Ö
- **–ü—Ä–∏—á–∏–Ω–∞**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ—Å–∞ + –ª–∏–Ω–µ–π–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
- **–†–µ—à–µ–Ω–∏–µ**: –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è + –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞

#### üü° –ü—Ä–æ–±–ª–µ–º–∞ #3: Humor detection = 0 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
- **–í–ª–∏—è–Ω–∏–µ**: –ù–µ–ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- **–ü—Ä–∏—á–∏–Ω–∞**: –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã —é–º–æ—Ä–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤–∏–¥–µ–æ
- **–†–µ—à–µ–Ω–∏–µ**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å ruBERT –∏–ª–∏ LLM

#### üü° –ü—Ä–æ–±–ª–µ–º–∞ #4: 15GB RAM –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
- **–í–ª–∏—è–Ω–∏–µ**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–∞ —Å–ª–∞–±—ã—Ö –º–∞—à–∏–Ω–∞—Ö
- **–ü—Ä–∏—á–∏–Ω–∞**: –í—Å–µ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏ + –≤–µ—Å—å –≤–∏–¥–µ–æ –±—É—Ñ–µ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω
- **–†–µ—à–µ–Ω–∏–µ**: Lazy loading + streaming processing

#### üü† –ü—Ä–æ–±–ª–µ–º–∞ #5: Slow pace_score (0.04-0.11)
- **–í–ª–∏—è–Ω–∏–µ**: –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –ø–æ—á—Ç–∏ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –∏—Ç–æ–≥–æ–≤—ã–π score
- **–ü—Ä–∏—á–∏–Ω–∞**: –ü–æ—Ä–æ–≥ –æ–∂–∏–¥–∞–µ–º—ã—Ö peaks –∑–∞–Ω–∏–∂–µ–Ω
- **–†–µ—à–µ–Ω–∏–µ**: –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

---

## –ß–∞—Å—Ç—å 2: –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –∫–∞–∂–¥–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü–†–û–ë–õ–ï–ú–ê 1: CUDA –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

#### –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
```
Device set to use cpu
‚ö†Ô∏è CUDA requested but not available, falling back to CPU
```

#### –ü—Ä–∏—á–∏–Ω—ã –≤ –ø–æ—Ä—è–¥–∫–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
1. **PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–µ–∑ CUDA** (CPU-only –≤–µ—Ä—Å–∏—è)
2. **CUDA Toolkit –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞ –º–∞—à–∏–Ω—É** (–Ω–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ, —Ç–∫ –µ—Å—Ç—å –æ—à–∏–±–∫–∞ –æ fallback)
3. **–ö–æ–Ω—Ñ–ª–∏–∫—Ç –≤–µ—Ä—Å–∏–π** (CUDA 12.x vs PyTorch –æ–∂–∏–¥–∞–µ—Ç 11.8)
4. **–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è** –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã

#### –†–µ—à–µ–Ω–∏–µ (–ø–æ—à–∞–≥–æ–≤–æ–µ)

**–®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—É—â—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É**
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

**–®–∞–≥ 2: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch –ø—Ä–∞–≤–∏–ª—å–Ω–æ**
```bash
# –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–π torch
pip uninstall torch torchvision torchaudio -y

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å CUDA 12.1 (—Å–∞–º–∞—è –Ω–æ–≤–∞—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# –ò–õ–ò –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ CUDA 11.8
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
```

**–®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Whisper –Ω–∞ CUDA**
```python
# –í speech_to_text_handler.py
from faster_whisper import WhisperModel

model = WhisperModel(
    "base",
    device="cuda",  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º CUDA
    compute_type="float16",  # –î–ª—è GPU (–±—ã—Å—Ç—Ä–æ + —Ç–æ—á–Ω–æ)
    num_workers=4  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
)
```

#### –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- STT: —Å 300s ‚Üí 40-60s (5-7x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
- –û–±—â–µ–µ –≤—Ä–µ–º—è: 385s ‚Üí 150-180s

---

### –ü–†–û–ë–õ–ï–ú–ê 2: –£–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω scores (0.53-0.57)

#### –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω—ã

**–¢–µ–∫—É—â–∞—è —Ñ–æ—Ä–º—É–ª–∞:**
```python
clip_score = (
    0.20 * norm(hook) +      # hook: 0.58-0.72
    0.15 * norm(pace) +      # pace: 0.04-0.11 ‚Üê –°–õ–ò–®–ö–û–ú –ù–ò–ó–ö–û!
    0.15 * norm(intensity) + # intensity: ~0.50-0.65
    0.15 * norm(clarity) +   # clarity: ~0.55-0.62
    0.15 * norm(emotion) +   # emotion: ~0.20-0.40
    0.10 * norm(boundary) +  # boundary: 0.20-0.50
    0.10 * norm(momentum)    # momentum: ~0.30-0.50
)
```

**–ü—Ä–æ–±–ª–µ–º–∞:** 
- –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –Ω–∏–∑–∫–∏–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (0.0-1.0 –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ dataset)
- –†–µ–∑—É–ª—å—Ç–∞—Ç: –ª–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è 7 —á–∏—Å–µ–ª (0.3-0.7) ‚Üí —É–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω

#### –†–µ—à–µ–Ω–∏–µ

**–í–∞—Ä–∏–∞–Ω—Ç A: –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```python
def enhanced_scoring(components: Dict[str, float]) -> float:
    """
    –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π scoring —Å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—è–º–∏.
    components: {
        'hook': 0.68,
        'pace': 0.05,
        'intensity': 0.60,
        'clarity': 0.54,
        'emotion': 0.30,
        'boundary': 0.40,
        'momentum': 0.40
    }
    """
    
    # 1. –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ non-linear transformations
    hook_s = components['hook'] ** 0.9  # –°–ª–∞–±–æ –Ω–∞–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–∞–±—ã–µ
    pace_s = min(1.0, components['pace'] * 3) ** 1.2  # –£—Å–∏–ª–∏–≤–∞–µ–º –Ω–∏–∑–∫–∏–µ
    intensity_s = (components['intensity'] - 0.3) / 0.7  # –ù–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
    clarity_s = (components['clarity'] ** 1.1) if components['clarity'] > 0.5 else components['clarity'] ** 0.8
    emotion_s = abs(components['emotion'] - 0.5) * 2  # –£—Å–∏–ª–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º—É–º—ã
    boundary_s = components['boundary'] ** 0.8
    momentum_s = components['momentum'] ** 1.1
    
    # 2. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    base_score = (
        0.30 * hook_s +      # Hook ‚Äî —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π
        0.20 * clarity_s +   # Clarity ‚Äî –≤–∞–∂–Ω–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
        0.15 * intensity_s + # Intensity ‚Äî —ç–Ω–µ—Ä–≥–∏—è
        0.15 * emotion_s +   # Emotion ‚Äî —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        0.10 * momentum_s +  # Momentum ‚Äî –¥—É–≥–∞
        0.05 * boundary_s +  # Boundary ‚Äî –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å
        0.05 * pace_s        # Pace ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞
    )
    
    # 3. –ë–æ–Ω—É—Å—ã –∑–∞ —Å–æ—á–µ—Ç–∞–Ω–∏—è
    bonuses = 0
    
    # –ë–æ–Ω—É—Å: —Å–∏–ª—å–Ω—ã–π hook + strong intensity
    if hook_s > 0.6 and intensity_s > 0.6:
        bonuses += 0.05
    
    # –ë–æ–Ω—É—Å: —Ö–æ—Ä–æ—à–æ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    good_components = sum(1 for v in [hook_s, clarity_s, intensity_s] if v > 0.6)
    bonuses += good_components * 0.02
    
    # –®—Ç—Ä–∞—Ñ: —Å–ª–∞–±—ã–π hook
    if hook_s < 0.3:
        bonuses -= 0.10
    
    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å (S-curve –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞)
    final = base_score + bonuses
    # Sigmoid-like transformation
    final = 1 / (1 + np.exp(-5 * (final - 0.5)))  # –†–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç 0.3-0.7 –≤ 0.1-0.9
    
    return np.clip(final, 0, 1)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –î–∏–∞–ø–∞–∑–æ–Ω —Ä–∞—Å—à–∏—Ä–∏—Ç—Å—è —Å [0.53-0.57] –Ω–∞ [0.25-0.85]

**–í–∞—Ä–∏–∞–Ω—Ç B: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞** (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
```python
def adaptive_weights(components: Dict[str, float]) -> Dict[str, float]:
    """–í–µ—Å–∞ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –≤–∏–¥–µ–æ-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    
    # –ï—Å–ª–∏ —ç—Ç–æ –æ—á–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∏–¥–µ–æ
    if components['motion_peak'] > 0.7:
        return {
            'hook': 0.25,
            'intensity': 0.25,  # –£—Å–∏–ª–∏–≤–∞–µ–º –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ
            'clarity': 0.15,
            'emotion': 0.15,
            'pace': 0.10,
            'boundary': 0.05,
            'momentum': 0.05,
        }
    
    # –ï—Å–ª–∏ —ç—Ç–æ –¥–∏–∞–ª–æ–≥/—Ä–∞–∑–≥–æ–≤–æ—Ä
    elif components['dialogue_ratio'] > 0.7:
        return {
            'hook': 0.25,
            'clarity': 0.25,  # –£—Å–∏–ª–∏–≤–∞–µ–º –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
            'emotion': 0.20,
            'intensity': 0.15,
            'pace': 0.10,
            'boundary': 0.03,
            'momentum': 0.02,
        }
    
    # Default
    else:
        return {
            'hook': 0.25,
            'intensity': 0.20,
            'clarity': 0.20,
            'emotion': 0.15,
            'pace': 0.10,
            'boundary': 0.05,
            'momentum': 0.05,
        }
```

---

### –ü–†–û–ë–õ–ï–ú–ê 3: Humor detection = 0

#### –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–±–ª–µ–º—ã
```python
# –¢–µ–∫—É—â–∏–π –∫–æ–¥ (–Ω–µ—Ä–∞–±–æ—Ç–∞–µ—Ç)
HUMOR_MARKERS = [
    "funny", "hilarious", "lol", "haha", "comic"
]

def detect_humor(text: str):
    text_lower = text.lower()
    return any(marker in text_lower for marker in HUMOR_MARKERS)
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í—Å–µ –º–∞—Ä–∫–µ—Ä—ã –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ, –≤–∏–¥–µ–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º!

#### –†–µ—à–µ–Ω–∏–µ 1: –†—É—Å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–±—ã—Å—Ç—Ä–æ)
```python
# humor_detection_handler.py
from typing import List, Dict

# –†—É—Å—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã
RUSSIAN_HUMOR_MARKERS = {
    'laughing': ['—Ö–∞—Ö–∞—Ö–∞', '–∞—Ö–∞—Ö–∞—Ö–∞', '–∞—Ö–∞—Ö–∞', '—Ö–∞-—Ö–∞', '—Ö–µ-—Ö–µ', '—Ö–∏—Ö–∏'],
    'explicit': ['—Å–º–µ—à–Ω–æ', '–ø—Ä–∏–∫–æ–ª', '—É–≥–∞—Ä', '–æ—Ä—É', '—Ä–∂—É', '—É–º–∏—Ä–∞—é', '–∂–µ—Å—Ç—å'],
    'slang': ['–ª–æ–ª', '–∫–µ–∫', '–∫–µ–∫–≤', '–∞—É—Ñ', '–æ—Ä—É', '–±—É–ª—å'],
    'irony': ['–∫–æ–Ω–µ—á–Ω–æ', '—Ç–∏–ø–∞', '—è–∫–æ–±—ã', '–º–æ–ª', '–≤–æ—Ç —ç—Ç–æ –¥–∞'],
    'sarcasm': ['—Å—É–ø–µ—Ä', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–∫–ª–∞—Å—Å–Ω–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–æ–≥–æ–Ω—å'],
    'emotion_markers': ['üòÇ', 'ü§£', 'üòÜ', 'üòÖ', 'üòÑ', '‚ò†Ô∏è', 'üíÄ']
}

def detect_humor_russian(text: str, sentiment_data: Optional[List[float]] = None) -> float:
    """
    –î–µ—Ç–µ–∫—Ü–∏—è —é–º–æ—Ä–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    Returns: confidence score 0-1
    """
    text_lower = text.lower()
    confidence = 0
    
    # 1. –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤
    marker_matches = 0
    for category, markers in RUSSIAN_HUMOR_MARKERS.items():
        for marker in markers:
            if marker in text_lower:
                marker_matches += 1
                confidence += 0.15
    
    # 2. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å sentiment)
    if sentiment_data and len(sentiment_data) > 0:
        sentiment_changes = sum(1 for i in range(1, len(sentiment_data)) 
                               if (sentiment_data[i] > 0.5) and (sentiment_data[i-1] < 0))
        if sentiment_changes > 0:
            confidence += sentiment_changes * 0.1
    
    # 3. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    # –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø–∞—É–∑—É –¥–ª—è —Å–º–µ—Ö–∞
    ellipsis_count = text.count('...')
    if ellipsis_count > text.count('.') / 3:
        confidence += 0.05
    
    # –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏
    exclamation_ratio = text.count('!') / max(len(text.split()), 1)
    if exclamation_ratio > 0.1:
        confidence += 0.1
    
    return min(confidence, 1.0)


# –í –æ—Å–Ω–æ–≤–Ω–æ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ
class HumorDetectionHandler(BaseHandler):
    def handle(self, state: AnalysisState) -> AnalysisState:
        humor_scores = []
        
        for segment in state.speechToText.segments:
            confidence = detect_humor_russian(
                segment.text,
                sentiment_data=state.sentiment.segmentSentiments  # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            )
            humor_scores.append({
                'segment_index': segment.index,
                'start': segment.start,
                'end': segment.end,
                'humor_score': confidence,
                'text_excerpt': segment.text[:50]
            })
        
        state.humor = HumorDetectionResult(
            scores=humor_scores,
            summary={'mean': np.mean([s['humor_score'] for s in humor_scores]),
                     'max': max([s['humor_score'] for s in humor_scores]),
                     'count_positive': sum(1 for s in humor_scores if s['humor_score'] > 0.3)}
        )
        return state
```

#### –†–µ—à–µ–Ω–∏–µ 2: LLM-based (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å)
```python
async def detect_humor_with_llm(
    segments: List[TranscriptSegment],
    model: str = "gpt-4-mini"
) -> List[float]:
    """
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —é–º–æ—Ä–∞ –≤ —Ä—É—Å—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
    """
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –≤ –±–∞—Ç—á–∏ (—ç–∫–æ–Ω–æ–º–∏–º API calls)
    batch_size = 10
    scores = []
    
    for i in range(0, len(segments), batch_size):
        batch = segments[i:i+batch_size]
        batch_text = "\n\n".join([
            f"{j}. ({s.start:.0f}s) {s.text[:100]}"
            for j, s in enumerate(batch)
        ])
        
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —é–º–æ—Ä–∞ –∏ –∑–∞–±–∞–≤–Ω–æ—Å—Ç–∏.
–û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ø–æ —à–∫–∞–ª–µ 0-1 (0 = —Å–µ—Ä—å—ë–∑–Ω–æ, 1 = –æ—á–µ–Ω—å —Å–º–µ—à–Ω–æ).

{batch_text}

–í–µ—Ä–Ω–∏ JSON —Å –º–∞—Å—Å–∏–≤–æ–º –æ—Ü–µ–Ω–æ–∫:
{{"scores": [0.0, 0.3, 0.8, ...]}}
"""
        
        response = await openai.ChatCompletion.acreate(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        batch_scores = json.loads(response.choices[0].message.content)['scores']
        scores.extend(batch_scores)
    
    return scores
```

---

### –ü–†–û–ë–õ–ï–ú–ê 4: 15GB RAM –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è

#### –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
```
Model weights:      ~4-5 GB
  - Whisper base    ~1.5 GB
  - YAMNet          ~0.5 GB
  - BERT/distilbert ~1.5 GB
  - Other           ~1 GB

Runtime buffers:    ~8-10 GB
  - TensorFlow      ~2-3 GB
  - Audio (WAV)     ~2 GB
  - OpenCV frames   ~2-3 GB
  - NumPy arrays    ~1 GB
  - Python          ~1 GB

TOTAL:              ~15 GB
```

#### –†–µ—à–µ–Ω–∏–µ 1: Lazy Loading (—ç–∫–æ–Ω–æ–º–∏—Ç 4-5 GB)
```python
# config/model_manager.py

import gc
from typing import Dict, Any, Optional
import torch

class LazyModelManager:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω—É–∂–Ω—ã.
    –û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    """
    
    def __init__(self):
        self._models: Dict[str, Any] = {}
        self._last_used: Dict[str, float] = {}
    
    def get_model(self, model_name: str, device: str = "cuda") -> Any:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –ó–∞–≥—Ä—É–∂–∞–µ—Ç, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.
        model_name: "whisper", "bert", "yamnet"
        """
        
        if model_name in self._models:
            self._last_used[model_name] = time.time()
            return self._models[model_name]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        if model_name == "whisper":
            from faster_whisper import WhisperModel
            model = WhisperModel("base", device=device, compute_type="float16")
        
        elif model_name == "bert":
            from transformers import AutoModelForSequenceClassification
            model = AutoModelForSequenceClassification.from_pretrained(
                "distilbert-base-uncased-finetuned-sst-2-english",
                device_map=device,
                low_cpu_mem_usage=True
            )
        
        elif model_name == "yamnet":
            import tensorflow_hub as hub
            model = hub.load("https://tfhub.dev/google/yamnet/1")
        
        self._models[model_name] = model
        self._last_used[model_name] = time.time()
        
        return model
    
    def cleanup_old_models(self, max_age_minutes: int = 30):
        """–£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏."""
        current_time = time.time()
        to_remove = []
        
        for model_name, last_used in self._last_used.items():
            if (current_time - last_used) > max_age_minutes * 60:
                to_remove.append(model_name)
        
        for model_name in to_remove:
            del self._models[model_name]
            del self._last_used[model_name]
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return len(to_remove)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å
MODEL_MANAGER = LazyModelManager()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ handlers
class SentimentAnalysisHandler(BaseHandler):
    def handle(self, state: AnalysisState) -> AnalysisState:
        model = MODEL_MANAGER.get_model("bert", device=state.globalConfig.device)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞...
        results = model.predict(texts)
        
        # –ü–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏
        MODEL_MANAGER.cleanup_old_models(max_age_minutes=15)
        
        return state
```

#### –†–µ—à–µ–Ω–∏–µ 2: Streaming Audio Processing (—ç–∫–æ–Ω–æ–º–∏—Ç 2 GB)
```python
# features/audio/audio_features_handler.py

def extract_features_streaming(
    audio_path: str,
    chunk_duration: float = 30.0,  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 30 —Å–µ–∫
    sr: int = 16000
) -> List[AudioFeaturePoint]:
    """
    –ü–æ—Ç–æ–∫–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ –∞—É–¥–∏–æ –≤ –ø–∞–º—è—Ç—å.
    """
    import librosa
    import soundfile as sf
    
    features = []
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏
    info = sf.info(audio_path)
    total_frames = info.frames
    chunk_frames = int(chunk_duration * sr)
    
    # –ß–∏—Ç–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∞–º–∏
    with sf.SoundFile(audio_path) as f:
        for start_frame in range(0, total_frames, chunk_frames):
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π —á–∞–Ω–∫
            audio_chunk = f.read(chunk_frames)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
            chunk_features = librosa.feature.mfcc(y=audio_chunk, sr=sr, n_mfcc=13)
            loudness = np.sqrt(np.mean(audio_chunk ** 2))
            energy = np.sum(audio_chunk ** 2) / len(audio_chunk)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            time = start_frame / sr
            features.append(AudioFeaturePoint(
                time=time,
                loudness=loudness,
                energy=energy,
                mfcc=chunk_features.mean(axis=1).tolist()
            ))
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
            del audio_chunk, chunk_features
            gc.collect()
    
    return features
```

#### –†–µ—à–µ–Ω–∏–µ 3: Chunk-based Video Processing (–¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ)
```python
# pipeline/chunk_processor.py

def process_video_in_chunks(
    video_path: str,
    chunk_duration: int = 600,  # 10 –º–∏–Ω—É—Ç –Ω–∞ —á–∞–Ω–∫
) -> AnalysisResult:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ —á–∞–Ω–∫–∞–º–∏ –ø–æ 10 –º–∏–Ω—É—Ç.
    –û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –ø–∞–º—è—Ç—å –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏.
    """
    
    total_duration = get_video_duration(video_path)
    chunk_results = []
    
    for start_sec in range(0, int(total_duration), chunk_duration):
        end_sec = min(start_sec + chunk_duration, int(total_duration))
        
        # –û–±—Ä–µ–∑–∞—Ç—å –≤–∏–¥–µ–æ –Ω–∞ —á–∞–Ω–∫
        chunk_path = extract_video_segment(video_path, start_sec, end_sec)
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —á–∞–Ω–∫
        chunk_state = AnalysisState(
            videoPath=chunk_path,
            analysisId=f"chunk_{start_sec}_{end_sec}",
            globalConfig=GLOBAL_CONFIG,
            # ...
        )
        
        # –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —á–∞–Ω–∫–∞
        result = run_dag_pipeline(chunk_state)
        chunk_results.append((start_sec, result))
        
        # –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å
        os.remove(chunk_path)
        gc.collect()
        torch.cuda.empty_cache()
    
    # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Å–¥–≤–∏–≥–æ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    final_result = merge_chunk_results(chunk_results, chunk_duration)
    return final_result

def merge_chunk_results(
    chunk_results: List[Tuple[int, AnalysisResult]],
    chunk_duration: int
) -> AnalysisResult:
    """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞–Ω–∫–æ–≤."""
    
    merged = AnalysisResult()
    
    for offset, chunk_result in chunk_results:
        # –°–¥–≤–∏–≥–∞–µ–º –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        for timeline_point in chunk_result.timeline.timeline:
            timeline_point.time += offset
        
        merged.timeline.timeline.extend(chunk_result.timeline.timeline)
        
        # –°–¥–≤–∏–≥–∞–µ–º —Ö–∞–π–ª–∞–π—Ç—ã
        for clip in chunk_result.highlights.clips:
            clip.startSeconds += offset
            clip.endSeconds += offset
        
        merged.highlights.clips.extend(chunk_result.highlights.clips)
    
    return merged
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** 15GB ‚Üí 8-10GB (-33% –ø–∞–º—è—Ç–∏)

---

### –ü–†–û–ë–õ–ï–ú–ê 5: –ù–∏–∑–∫–∏–π pace_score (0.04-0.11)

#### –ê–Ω–∞–ª–∏–∑
```python
# –¢–µ–∫—É—â–∏–π —Ä–∞—Å—á—ë—Ç:
peaks_in_window = count(local_peaks in interest[start:end])
expected_peaks ~ 1 –ø–∏–∫ –Ω–∞ 10 —Å–µ–∫
pace = peaks_in_window / (duration_sec / 10)

# –î–ª—è 60-—Å–µ–∫—É–Ω–¥–Ω–æ–≥–æ –æ–∫–Ω–∞:
expected_peaks = 60 / 10 = 6
actual_peaks = 0.5 (–≤ —Å—Ä–µ–¥–Ω–µ–º) ‚Üê –û–ß–ï–ù–¨ –ù–ò–ó–ö–û!
pace = 0.5 / 6 = 0.083
```

#### –ü—Ä–∏—á–∏–Ω–∞
Timeline —Å–ª–∏—à–∫–æ–º —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–π (1-sec —à–∞–≥). –ü–∏–∫–∏ —Ä–∞–∑–º—ã–≤–∞—é—Ç—Å—è.

#### –†–µ—à–µ–Ω–∏–µ 1: –ò—Å–ø–æ–ª—å–∑—É–π –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–∏
```python
# –í–º–µ—Å—Ç–æ 1-sec timeline –¥–µ–ª–∞–µ–º 0.1-sec (100ms)
def create_high_res_timeline(
    motion: List[float],  # per frame (33ms)
    audio_loudness: List[float],  # per frame
    fps: float = 30,
    target_step: float = 0.1  # 100ms
) -> List[TimelinePoint]:
    """
    –°–æ–∑–¥–∞—Ç—å –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —à–∫–∞–ª—É –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–∏–∫–æ–≤.
    """
    
    frame_step = int(fps * target_step)  # —Å–∫–æ–ª—å–∫–æ —Ñ—Ä–µ–π–º–æ–≤ –≤ 100ms
    
    timeline = []
    for i in range(0, len(motion), frame_step):
        window_end = min(i + frame_step, len(motion))
        
        time_sec = i / fps
        motion_val = np.mean(motion[i:window_end])
        loudness_val = np.mean(audio_loudness[i:window_end])
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º...
        
        timeline.append(TimelinePoint(
            time=time_sec,
            motion=motion_val,
            audioLoudness=loudness_val,
            # ...
        ))
    
    return timeline
```

#### –†–µ—à–µ–Ω–∏–µ 2: –õ—É—á—à–µ —Å—á–∏—Ç–∞–π –ø–∏–∫–∏
```python
def find_local_peaks(
    signal: np.ndarray,
    height_percentile: float = 75,  # top 25%
    min_distance: int = 3  # –º–∏–Ω–∏–º—É–º 0.3 —Å–µ–∫ –º–µ–∂–¥—É –ø–∏–∫–∞–º–∏
) -> List[int]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–∏–∫–∏ –≤ —Å–∏–≥–Ω–∞–ª–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º.
    """
    from scipy.signal import find_peaks
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
    threshold = np.percentile(signal, height_percentile)
    
    peaks, properties = find_peaks(
        signal,
        height=threshold,
        distance=min_distance,
        prominence=np.std(signal) * 0.5
    )
    
    return peaks

def calculate_pace_score(
    timeline: List[TimelinePoint],
    start_idx: int,
    end_idx: int
) -> float:
    """–ü–µ—Ä–µ–¥–µ–ª–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç pace."""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ç–µ—Ä–µ—Å –¥–ª—è –æ–∫–Ω–∞
    interest = np.array([p.interest for p in timeline[start_idx:end_idx]])
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏
    peaks = find_local_peaks(interest, height_percentile=70)
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
    duration_sec = (end_idx - start_idx) * 0.1  # –µ—Å–ª–∏ timeline –Ω–∞ 100ms
    expected_peaks = max(1, duration_sec / 5)  # 1 –ø–∏–∫ –Ω–∞ 5 —Å–µ–∫ (–±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ)
    
    pace = min(len(peaks) / expected_peaks, 2.0)  # cap –Ω–∞ 2.0 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    
    return min(pace, 1.0)
```

#### –†–µ—à–µ–Ω–∏–µ 3: –ü–µ—Ä–µ–æ—Ü–µ–Ω–∏ –≤–µ—Å–∞
```python
def enhanced_scoring_v2(components: Dict[str, float]) -> float:
    """
    –° –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –¥–ª—è —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ pace.
    """
    
    hook_s = components['hook'] ** 0.9
    pace_s = (components['pace'] - 0.2) / 0.8  # –ü–µ—Ä–µ–æ—Ü–µ–Ω—Ç–∏—Ä—É–µ–º (–±—ã–ª–æ 0.04-0.11 ‚Üí 0.0-1.0)
    intensity_s = components['intensity'] ** 1.0
    clarity_s = components['clarity'] ** 1.1
    emotion_s = abs(components['emotion'] - 0.5) * 2
    boundary_s = components['boundary'] ** 0.8
    momentum_s = components['momentum'] ** 1.1
    
    score = (
        0.28 * hook_s +      # Hook ‚Äî –∫–ª—é—á–µ–≤–æ–π
        0.18 * clarity_s +   # Clarity
        0.15 * intensity_s + # Intensity
        0.15 * emotion_s +   # Emotion
        0.12 * pace_s +      # Pace ‚Äî —Ç–µ–ø–µ—Ä—å –∑–Ω–∞—á–∏–º—ã–π!
        0.07 * boundary_s +
        0.05 * momentum_s
    )
    
    return np.clip(score, 0, 1)
```

---

## –ß–∞—Å—Ç—å 3: Roadmap —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –§–∞–∑–∞ 1: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø (1-2 –¥–Ω—è) üî¥

```
[ ] 1.1. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
        - Command: pip install torch CUDA 12.1
        - –¢–µ—Å—Ç: CUDA available –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å True
        - –†–µ–∑—É–ª—å—Ç–∞—Ç: STT —É—Å–∫–æ—Ä–∏—Ç—Å—è –≤ 5x

[ ] 1.2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Whisper –Ω–∞ GPU
        - –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å speech_to_text_handler.py
        - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å float16 –Ω–∞ GPU
        - –¢–µ—Å—Ç: Whisper –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CUDA

[ ] 1.3. –î–æ–±–∞–≤–∏—Ç—å —Ä—É—Å—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã —é–º–æ—Ä–∞
        - –î–æ–±–∞–≤–∏—Ç—å RUSSIAN_HUMOR_MARKERS —Å–ª–æ–≤–∞—Ä—å
        - –û–±–Ω–æ–≤–∏—Ç—å detect_humor —Ñ—É–Ω–∫—Ü–∏—é
        - –¢–µ—Å—Ç: humor_count > 0 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ

[ ] 1.4. –ò—Å–ø—Ä–∞–≤–∏—Ç—å pace_score
        - –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–µ –ø–∏–∫–∏ (1 –Ω–∞ 5 —Å–µ–∫ –≤–º–µ—Å—Ç–æ 1 –Ω–∞ 10)
        - –î–æ–±–∞–≤–∏—Ç—å find_local_peaks —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        - –¢–µ—Å—Ç: pace_score –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.3-0.8

–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 385s ‚Üí 100-120s (3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
```

### –§–∞–∑–∞ 2: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ (3-5 –¥–Ω–µ–π) üü°

```
[ ] 2.1. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å LazyModelManager
        - –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏
        - Lazy loading + cleanup
        - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤–æ –≤—Å–µ handlers

[ ] 2.2. Streaming audio processing
        - –ó–∞–º–µ–Ω–∏—Ç—å librosa.load() –Ω–∞ chunk processing
        - –¢–µ—Å—Ç: memory usage < 5GB –¥–ª—è –∞—É–¥–∏–æ

[ ] 2.3. Chunk-based video processing
        - –î–ª—è –≤–∏–¥–µ–æ > 60 –º–∏–Ω —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
        - Merge —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 15GB ‚Üí 8GB (-47% –ø–∞–º—è—Ç—å)
```

### –§–∞–∑–∞ 3: –£–ª—É—á—à–µ–Ω–∏–µ scoring (1 –Ω–µ–¥–µ–ª—è) üü†

```
[ ] 3.1. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å enhanced_scoring —Å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—è–º–∏
        - –ó–∞–º–µ–Ω–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
        - –î–æ–±–∞–≤–∏—Ç—å bonuses + penalties
        - –¢–µ—Å—Ç: –¥–∏–∞–ø–∞–∑–æ–Ω scores –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å [0.25-0.85]

[ ] 3.2. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∏–¥–µ–æ
        - Detect –≤–∏–¥–µ–æ-—Ç–∏–ø–∞ (–∞–∫—Ç–∏–≤–Ω–æ–µ/–¥–∏–∞–ª–æ–≥/–º–∏–∫—Å)
        - Apply —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞
        - –¢–µ—Å—Ç: scores –¥–æ–ª–∂–Ω—ã –ª—É—á—à–µ —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ

[ ] 3.3. –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        - –õ—É—á—à–µ —Å—á–∏—Ç–∞—Ç—å overlap penalty
        - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å smart refit (–Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–¥–≤–∏–≥, –∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)

–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª–∏–ø—ã –±—É–¥—É—Ç –∑–∞–º–µ—Ç–Ω–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å—Å—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
```

### –§–∞–∑–∞ 4: Advanced Features (2-3 –Ω–µ–¥–µ–ª–∏) üü¢

```
[ ] 4.1. LLM-refinement (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - Batch API calls –¥–ª—è GPT-4
        - –ê–Ω–∞–ª–∏–∑ top-30 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        - –ü—Ä–æ–≤–µ—Ä–∫–∞ "–ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"

[ ] 4.2. Multimodal analysis
        - Gemini 1.5 –∏–ª–∏ GPT-4V –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–¥—Ä–æ–≤
        - –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü, —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ
        - –°–∏–Ω—Ç–µ–∑ —Å audio –∞–Ω–∞–ª–∏–∑–æ–º

[ ] 4.3. Web UI –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        - Streamlit –∏–ª–∏ FastAPI + React
        - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Ö–∞–π–ª–∞–π—Ç–æ–≤
        - –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã

[ ] 4.4. Production deployment
        - Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
        - Task queue (Celery)
        - API endpoint
```

---

## –ß–∞—Å—Ç—å 4: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤

### –§–∞–π–ª: `features/config/initialization_handler.py` (–ù–û–í–´–ô)

```python
# –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å CUDA detection

class InitializationHandler(BaseHandler):
    def handle(self, state: AnalysisState) -> AnalysisState:
        # CUDA detection
        device, cuda_info = self._detect_cuda()
        
        # –í—ã–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        if device == "cuda" and cuda_info.get("cuda_memory_mb", 0) >= 8000:
            whisper_size = "small"  # –ù–∞ GPU small –±—É–¥–µ—Ç –±—ã—Å—Ç—Ä–æ
        elif device == "cuda":
            whisper_size = "base"
        else:
            whisper_size = "tiny"  # –ù–∞ CPU —Ç–æ–ª—å–∫–æ tiny
        
        state.globalConfig = GlobalConfig(
            device=device,
            gpu_available=(device == "cuda"),
            whisper_model_size=whisper_size,
            cuda_memory_mb=cuda_info.get("cuda_memory_mb"),
            # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        )
        
        return state

    @staticmethod
    def _detect_cuda() -> Tuple[Literal["cuda", "cpu"], Dict]:
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda", {
                    "version": torch.version.cuda,
                    "cuda_memory_mb": torch.cuda.get_device_properties(0).total_memory // (1024**2),
                    "gpu_name": torch.cuda.get_device_name(0),
                }
            else:
                return "cpu", {}
        except ImportError:
            return "cpu", {}
```

### –§–∞–π–ª: `features/nlp/humor_detection_handler.py` (–ü–ï–†–ï–î–ï–õ–ê–ù)

```python
class HumorDetectionHandler(BaseHandler):
    RUSSIAN_HUMOR_MARKERS = {
        'laughing': ['—Ö–∞—Ö–∞—Ö–∞', '–∞—Ö–∞—Ö–∞—Ö–∞', '–∞—Ö–∞—Ö–∞', '—Ö–µ-—Ö–µ', '—Ö–∏—Ö–∏'],
        'explicit': ['—Å–º–µ—à–Ω–æ', '–ø—Ä–∏–∫–æ–ª', '—É–≥–∞—Ä', '–æ—Ä—É', '—Ä–∂—É', '–∂–µ—Å—Ç—å'],
        'slang': ['–ª–æ–ª', '–∫–µ–∫', '–∞—É—Ñ', '–±—É–ª—å'],
        'emotion': ['üòÇ', 'ü§£', 'üòÜ', 'üòÖ'],
    }
    
    def handle(self, state: AnalysisState) -> AnalysisState:
        humor_scores = []
        
        for segment in state.speechToText.segments:
            confidence = self._detect_humor_russian(
                segment.text,
                segment.start,
                segment.end,
                state.sentiment  # –ò—Å–ø–æ–ª—å–∑—É–µ–º sentiment –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è
            )
            
            humor_scores.append({
                'segment': segment.index,
                'score': confidence,
                'start': segment.start,
                'end': segment.end,
            })
        
        state.humor = HumorDetectionResult(
            scores=humor_scores,
            summary={
                'mean': np.mean([s['score'] for s in humor_scores]),
                'max': max([s['score'] for s in humor_scores]),
                'count_positive': sum(1 for s in humor_scores if s['score'] > 0.3),
            }
        )
        
        return state
    
    def _detect_humor_russian(self, text: str, start: float, end: float, sentiment_data) -> float:
        text_lower = text.lower()
        confidence = 0
        
        # –ú–∞—Ä–∫–µ—Ä-–º–∞—Ç—á–∏–Ω–≥
        for category, markers in self.RUSSIAN_HUMOR_MARKERS.items():
            confidence += sum(0.2 for m in markers if m in text_lower)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        confidence += text.count('!') * 0.05
        confidence += text.count('...') * 0.03
        
        # Sentiment boost (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–∫–∞—á–∫–∏ —ç–º–æ—Ü–∏–π ‚Üí —Å–º–µ—à–Ω–æ)
        if sentiment_data:
            seg_sentiment = [s for s in sentiment_data.segmentSentiments 
                           if s.start >= start and s.end <= end]
            if seg_sentiment:
                changes = sum(1 for i in range(1, len(seg_sentiment)) 
                            if seg_sentiment[i].sentiment * seg_sentiment[i-1].sentiment < 0)
                confidence += changes * 0.15
        
        return min(confidence, 1.0)
```

### –§–∞–π–ª: `features/highlights/viral_moments_handler.py` (–ü–ï–†–ï–î–ï–õ–ê–ù)

```python
class ViralMomentsHandler(BaseHandler):
    def handle(self, state: AnalysisState) -> AnalysisState:
        # –ü–æ–ª—É—á–∏—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ CandidateSelectionHandler
        candidates = state.candidates
        
        # –°–∫–æ—Ä–∏—Ç—å –≤—Å–µ—Ö
        scored_candidates = []
        for candidate in candidates:
            score_breakdown = self._score_candidate(candidate, state.timeline.timeline)
            scored_candidates.append({
                'candidate': candidate,
                'score': score_breakdown['final_score'],
                'breakdown': score_breakdown,
            })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score
        scored_candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + selection
        selected = self._select_diverse_clips(scored_candidates)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        state.highlights = HighlightDetectionResult(
            clips=[self._format_clip(c) for c in selected],
            summary={'total_selected': len(selected), 'total_candidates': len(candidates)}
        )
        
        return state
    
    def _score_candidate(self, candidate, timeline) -> Dict:
        """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —Å–∫–æ—Ä–∏–Ω–≥ —Å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—è–º–∏."""
        
        start_idx = int(candidate.start_seconds)
        end_idx = int(candidate.end_seconds)
        window = timeline[start_idx:end_idx]
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        hook = np.mean([p.interest for p in window[:3]])
        clarity = np.mean([p.clarity for p in window])
        intensity = 0.6 * np.mean([p.motion for p in window]) + 0.4 * np.mean([p.audioLoudness for p in window])
        emotion = abs(np.mean([p.sentiment for p in window]))
        
        # –ü–∏–∫–∏ –≤ –æ–∫–Ω–µ
        interest_vals = np.array([p.interest for p in window])
        peaks = find_local_peaks(interest_vals)
        pace = len(peaks) / max(1, len(window) / 5)
        
        # Momentum
        first_half_intensity = np.mean([p.audioLoudness for p in window[:len(window)//2]])
        second_half_intensity = np.mean([p.audioLoudness for p in window[len(window)//2:]])
        momentum = second_half_intensity / max(first_half_intensity, 0.1)
        
        # Boundaries
        boundary_score = 0
        if window[0].isSceneBoundary:
            boundary_score += 0.5
        if window[-1].isSceneBoundary:
            boundary_score += 0.5
        
        # –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        hook_s = hook ** 0.9
        clarity_s = clarity ** 1.1
        intensity_s = (intensity - 0.3) / 0.7 if intensity > 0.3 else 0
        emotion_s = emotion ** 1.0
        pace_s = min(pace / 2, 1.0)
        momentum_s = momentum ** 1.1
        boundary_s = boundary_score ** 0.8
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏
        final_score = (
            0.30 * hook_s +
            0.22 * clarity_s +
            0.16 * intensity_s +
            0.15 * emotion_s +
            0.10 * pace_s +
            0.05 * boundary_s +
            0.02 * momentum_s
        )
        
        # Sigmoid –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        final_score = 1 / (1 + np.exp(-5 * (final_score - 0.5)))
        
        return {
            'final_score': final_score,
            'hook': hook_s,
            'clarity': clarity_s,
            'intensity': intensity_s,
            'emotion': emotion_s,
            'pace': pace_s,
            'boundary': boundary_s,
            'momentum': momentum_s,
        }
    
    def _select_diverse_clips(self, scored: List) -> List:
        """–í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–ª–∏–ø—ã —Å —à—Ç—Ä–∞—Ñ–æ–º –∑–∞ overlap."""
        
        selected = []
        used_regions = []
        
        for item in scored:
            candidate = item['candidate']
            score = item['score']
            
            # –í—ã—á–∏—Å–ª–∏—Ç—å overlap —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏
            max_overlap = 0
            for selected_item in selected:
                overlap = self._calculate_overlap(candidate, selected_item['candidate'])
                max_overlap = max(max_overlap, overlap)
            
            # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —à—Ç—Ä–∞—Ñ
            adjusted_score = score
            if max_overlap > 0.65:
                adjusted_score *= 0.3
            elif max_overlap > 0.35:
                adjusted_score *= 0.7
            
            # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë —Ö–æ—Ä–æ—à–∏–π –∏–ª–∏ —ç—Ç–æ –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç
            if adjusted_score > 0.35 or len(selected) < 5:
                selected.append(item)
                used_regions.append((candidate.start_seconds, candidate.end_seconds))
        
        return selected[:80]  # Top 80 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    
    def _calculate_overlap(self, c1, c2) -> float:
        """–°—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è."""
        inter = max(0, min(c1.end_seconds, c2.end_seconds) - max(c1.start_seconds, c2.start_seconds))
        union = max(c1.end_seconds - c1.start_seconds, c2.end_seconds - c2.start_seconds)
        return inter / union if union > 0 else 0
```

---

## –ß–∞—Å—Ç—å 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

### Metrics –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è

```python
class PerformanceMetrics:
    """–°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ –∏ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
    
    def __init__(self):
        self.metrics = {
            'processing_time': None,
            'ram_usage_peak': None,
            'gpu_utilized': False,
            'cuda_available': False,
            'score_range': (None, None),
            'humor_detected_count': 0,
            'pace_avg': 0,
        }
    
    def record(self, **kwargs):
        self.metrics.update(kwargs)
    
    def report(self):
        return f"""
PERFORMANCE REPORT
==================
Processing Time: {self.metrics['processing_time']:.1f}s
Peak RAM: {self.metrics['ram_usage_peak']:.1f}GB
GPU Used: {self.metrics['gpu_utilized']}
CUDA Available: {self.metrics['cuda_available']}
Score Range: {self.metrics['score_range'][0]:.2f}-{self.metrics['score_range'][1]:.2f}
Humor Detected: {self.metrics['humor_detected_count']}
Avg Pace Score: {self.metrics['pace_avg']:.3f}
"""
```

### –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç

```python
# tests/test_optimizations.py

def test_phase1_critical_fixes():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ñ–∞–∑—ã 1."""
    
    # 1. CUDA –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞
    assert torch.cuda.is_available(), "CUDA not available!"
    
    # 2. Whisper –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ GPU
    model = WhisperModel("base", device="cuda")
    assert model.model is not None
    
    # 3. Humor –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    text = "–≠—Ç–æ –æ—á–µ–Ω—å —Å–º–µ—à–Ω–æ! –ê—Ö–∞—Ö–∞—Ö–∞!"
    humor = detect_humor_russian(text)
    assert humor > 0.3
    
    # 4. Pace –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    # (—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)

def test_phase2_memory_optimization():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–∞–º—è—Ç–∏."""
    
    import psutil
    process = psutil.Process()
    
    # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
    initial_mem = process.memory_info().rss / 1024 / 1024  # MB
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω
    result = run_pipeline()
    
    # –ü–∏–∫–æ–≤–∞—è –ø–∞–º—è—Ç—å
    peak_mem = process.memory_info().rss / 1024 / 1024
    
    # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å < 10GB
    assert peak_mem < 10000, f"Memory usage too high: {peak_mem}MB"

def test_phase3_scoring_improvement():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ scoring."""
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ–æ
    result = run_pipeline(test_video_path)
    
    scores = [clip.score for clip in result.highlights.clips]
    
    # –î–∏–∞–ø–∞–∑–æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω
    score_range = max(scores) - min(scores)
    assert score_range > 0.3, f"Score range too narrow: {score_range}"
    
    # –ï—Å—Ç—å –∫–ª–∏–ø—ã —Ä–∞–∑–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    high = sum(1 for s in scores if s > 0.7)
    low = sum(1 for s in scores if s < 0.4)
    assert high > 0 and low > 0, "Scores not diverse enough"
```

---

## –ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã (—á—Ç–æ –¥–µ–ª–∞—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å):

1. **–ö–†–ò–¢–ò–ß–ù–û**: –ò—Å–ø—Ä–∞–≤–∏—Ç—å CUDA –¥–ª—è Whisper (1-2 —á–∞—Å–∞)
   - –≠—Ç–æ –¥–∞—Å—Ç 5x —É—Å–∫–æ—Ä–µ–Ω–∏–µ ‚Üí 385s ‚Üí 80s
   - –û—Å—Ç–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–∞ —ç—Ç–æ–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–µ

2. **–í–ê–ñ–ù–û**: –î–æ–±–∞–≤–∏—Ç—å —Ä—É—Å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç support (2-3 —á–∞—Å–∞)
   - Humor detection, sentiment –Ω–∞ —Ä—É—Å—Å–∫–æ–º

3. **–°–ï–†–¨–Å–ó–ù–û**: –£–ª—É—á—à–∏—Ç—å scoring (1-2 –¥–Ω—è)
   - –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
   - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
   - –≠—Ç–æ —É–ª—É—á—à–∏—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∏–ø–æ–≤

4. **–ü–û–õ–ï–ó–ù–û**: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å (3-5 –¥–Ω–µ–π)
   - LazyLoading, streaming processing
   - –ü—Ä–∏–≥–æ–¥–∏—Ç—Å—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞

5. **NICE-TO-HAVE**: LLM refinement (2-3 –Ω–µ–¥–µ–ª–∏)
   - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
